+++
title = "简明机器学习 - 可微分编程"
description = "Understanding differentiable Programming"
+++

得到一个程序有两种方式，其一是由程序员来编写它，其二是通过大量数据来拟合它。也就是所谓的编程和机器学习。

当我们在编程的时候，我们是在编写整个程序。当我们在机器学习的时候，我们在编写半个程序，剩下的部分，通过数据来拟合而得。

数据拟合，即通过数据，来反推这个产生这批数据的规律，这个规律相当于代码。

人脑的规则，和数据的规律，融合成一起，成为最终的程序。

现在问题来了，如何通过数据反推规律？

答案就在 “学习” 二字之中。

所谓学习，就是不断尝试，不断犯错，不断调整，直至不出错。

所谓机器学习，就是机器通过不断尝试，不断犯错，不断调整，直至不出错。

这个尝试，反馈，调整的回路，怎么搭建？

对于一个程序，哪些部分需要学习，哪些部分需要硬编码？

让我们从一个常量开始。

比如说，数字 `5`。

显然，常量没有任何变化可言，是个硬编码。

现在，我们来让这个常量产生变化的可能性，让它乘以一个变量 `w`。

```
w * 5
```

大功告成，世界上最简单的机器学习程序写好了。`5` 是人编写的，`w` 是未知的，需要通过机器学习而得到。

下一步，我们需要发明一种机制，来不断调整这个 `w`：

```
w = w - dw
```

`dw` 就是 `delta w`，也就是`w 的变化` 的意思。 `w = w - dw` 就是调整 `w`。

下一步，我们需要确定这个  `dw` 。

观察我们的程序 `w * 5`，它的意思是 `w` 的任何变化，都会被放大 `5` 倍。

假如我们把输出记为 `y`，那么 `dy = 5 * dw`。即：`dw = dy / 5`。

有了这个关系，当我们知道了 `dy`, 就等于知道了 `dw`，就等于知道了新的 `w`。

那么，如何知道 `dy`？

既然 `y` 是我们的输出，那么输出的误差，就是 `y` 需要调整的变化，即 `dy`：

```
dy = y_guess - y_true
```

其中 `y_true` 就是我们的数据（学习源），`y_guess` 是一次尝试的输出。`y_guess = w * 5` ，随 `w` 的变化而变化。

现在只差最后一步，我们就可以跑通这个流程了。这个最后一步是，`w` 的初始值怎么设置？

答案很简单，给个随机数就好了，反正它会被自动调节。

现在，我们用真实数据测试一下这个流程：

```
# --- test 1 ---
y = 15  # 正确答案, 即 w = 3
w = w0 = 2  # 随机初始化 w 为 2
y_guess = w * 5  # 2 * 5 = 10
dy = y_guess - y  # 10 - 15 = -5
dw = 1/5 * dy  # 1/5 * -5 = -1
w = w - dw  # 2 - (-1) = 3  <-- w = 3, y = 15 成功学习到 w = 3


# --- test 2 ---
y = 40  # 正确答案, 即 w = 8
w = w0 = 12  # 随机初始化 w 为 12
y_guess = w * 5  # 12 * 5 = 60
dy = y_guess - y  # 60 - 40 = 20
dw = 1/5 * dy  # 1/5 * 20 = 4
w = w - dw  # 12 - 4 = 8  <-- w = 8, y = 40 成功学习到 w = 8
```

我们从一个特例常量 `5` 开始，通过 `w * 5` 来推演这个机器学习的过程。其实把常量换成变量 `x`，上述的推演同样成立，也就是 `y = w * x`,  因为 `w` 的微小变化会被放大 `x`倍，所以  `dy = dw * x`,  `dw = 1/x * d*y`。通过特定的的 `(x, y)` 数据作为学习源，重复上述步骤，可得到正确的 `w`。

现在，我们研究出了怎么把变量和常量参数化，并且通过数据学习这个参数的技术。接下来我们更进一步：既然常量和变量都可以参数化，那么函数可不可以参数化？如果这个函数也是一个需要学习的函数呢？

```
h(x) = w1 * f(x)  # 参数化函数
f(x) = w2 * x  # 参数化变量
```

我们能同时学习得到 `w1` 和 `w2` 吗？

照着刚才的思路，我们需要先考察 `dw1` 和 `dw2` 。

对 `dw1`, 由 `w1 * f(x)` 可知 `w1` 的微小变化会被放大 `f(x)` 倍，所以 `d_h(x) =  dw1 * f(x)`。

对 `dw2`, 由 `w2 * x` 可知 `w2` 的微小变化会被放大 `x` 倍，所以 `d_f(x) = dw2 * x`； 又由 `w1 * f(x)` 可知，`f(x)` 的微小变化会被放大 `w1` 倍，所以 `d_h(x)  = d_f(x) * w1`。合并起来就是: `d_h(x) = d_f(x) * w1 = (dw2 * x) * w1` ，所以：

```
dw1 = d_h(x) / f(x) = d_h(x) / (w2 * x)
dw2 = d_h(x) / (x * w1)
```

我们按刚才的思路试一下：

```
--- 数据
设 w1 = 3, w2 = 6, x = 2, 则 f(x) = 6x, h(x) = 3f(x) = 18x
f(2) = 12, h(2) = 36

--- 随机初始化 w1, w2
w1 = 1, w2 = 4

--- 调整 10 个循环，结果如下：---
d_hx = -28, dw1:-3.5, dw2: -14.0, new w1: 4.5, new w2: 18.0
d_hx = 126.0, dw1:3.5, dw2: 14.0, new w1: 1.0, new w2: 4.0
d_hx = -28.0, dw1:-3.5, dw2: -14.0, new w1: 4.5, new w2: 18.0
d_hx = 126.0, dw1:3.5, dw2: 14.0, new w1: 1.0, new w2: 4.0
d_hx = -28.0, dw1:-3.5, dw2: -14.0, new w1: 4.5, new w2: 18.0
d_hx = 126.0, dw1:3.5, dw2: 14.0, new w1: 1.0, new w2: 4.0
d_hx = -28.0, dw1:-3.5, dw2: -14.0, new w1: 4.5, new w2: 18.0
d_hx = 126.0, dw1:3.5, dw2: 14.0, new w1: 1.0, new w2: 4.0
d_hx = -28.0, dw1:-3.5, dw2: -14.0, new w1: 4.5, new w2: 18.0
d_hx = 126.0, dw1:3.5, dw2: 14.0, new w1: 1.0, new w2: 4.0
```

显然，我们遇到了新问题：无论如何调整， `w1` 和 `w2` 只是在不停地来回震荡，无法学习到正确的值。

问题出在哪里？

问题不在于来回震荡，问题在于震荡幅度每次都一样大，使得这个震荡停不下来。一个钟摆来回摇摆，只要摆动幅度逐渐变小，那么它最终就会停下来。

所以灵感来了，让我们把震荡幅度每次都变小一点。从数学上表达：

```
w = w - dw * learning_rate
```

这个 `learning_rate` 原本是 `1` 被我们忽略掉了，当我们取一个`大于 0 小于 1 `的值时，震荡的幅度就会越来越小。

现在，取 `learning_rate = 0.6` 试试：

```
--- 数据
设 w1 = 3, w2 = 6, x = 2, 则 f(x) = 6x, h(x) = 3f(x) = 18x
f(2) = 12, h(2) = 36

--- 随机初始化 w1, w2
w1 = 1, w2 = 4

--- 调整 10 个循环，结果如下：---
d_hx = -28.000000, dw1:-3.5, dw2: -14.0, new w1: 3.1, new w2: 12.4
d_hx =  40.880000, dw1:1.6483870967741938, dw2: 6.593548387096775, new w1: 2.1109677419354838, new w2: 8.443870967741935
d_hx = -0.350522, dw1:-0.0207559744459344, dw2: -0.0830238977837376, new w1: 2.1234213266030446, new w2: 8.493685306412178
d_hx =  0.071345, dw1:0.004199887304937652, dw2: 0.016799549219750607, new w1: 2.120901394220082, new w2: 8.483605576880327
d_hx = -0.014218, dw1:-0.000837981435703175, dw2: -0.0033519257428127, new w1: 2.121404183081504, new w2: 8.485616732326015
d_hx =  0.002846, dw1:0.0001676757303203499, dw2: 0.0006707029212813996, new w1: 2.1213035776433116, new w2: 8.485214310573246
d_hx = -0.000569, dw1:-3.353196517289357e-05, dw2: -0.00013412786069157427, new w1: 2.121323696822415, new w2: 8.48529478728966
d_hx =  0.000114, dw1:6.706520244677623e-06, dw2: 2.6826080978710493e-05, new w1: 2.1213196729102686, new w2: 8.485278691641074
d_hx = -0.000023, dw1:-1.341298959827947e-06, dw2: -5.365195839311788e-06, new w1: 2.1213204776896446, new w2: 8.485281910758578
d_hx =  0.000005, dw1:2.682599957550061e-07, dw2: 1.0730399830200243e-06, new w1: 2.121320316733647, new w2: 8.485281266934589
```

成功了吗？

`d_hx` 也就是误差的确逐步降低到 0，可是学到的 `w1 = 2.121, w2 = 8.485` ，而我们正确答案是 `w1 = 3, w2 = 6`。怎么对不上呢？

细心分析下发现， `h(x) = w1 * f(x) = w1 * w2 * x  = (w1 * w2) * x` ，我们通过 `h(x)` 的误差来学习整个函数的表示，学到的是一个整体的结果，也就是 `w1 * w2`，在正确答案中，`w1 * w2  = 3 * 6 = 18`， 我们的实验结果：

`w1 * w2 = 2.121 * 8.484 = 17.994564 ~= 18` 也是对的。

也就是说，我们无意中得到了一个因式分解器，因 `w1, w2` 的初始值不同，我们学习得到不同的参数，这些参数结果对 `h(x)` 来说，效果都是一样的。有兴趣可以试试。